[{"content":"Introduction The foundations of information theory were laid by Shannon[1], building upon earlier work in computability by Turing[2].\nMathematical Formulation Shannon\u0026rsquo;s entropy is defined as:\n$$ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) $$\nThis can be related to Einstein\u0026rsquo;s[3] work in statistical mechanics.\nInline math also works: The energy-mass equivalence is $E = mc^2$.\nMore Complex Equations The Gaussian integral[1,3]:\n$$ \\int_{-\\infty}^{\\infty} e^{-\\frac{x^2}{2}} dx = \\sqrt{2\\pi} $$\nMatrix notation:\n$$ \\mathbf{A} = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \\ a_{21} \u0026amp; a_{22} \\end{bmatrix} $$\nReferences Claude E. Shannon. \"A Mathematical Theory of Communication\". Bell System Technical Journal, vol. 27, pp. 379-423, 1948. Alan M. Turing. \"On computable numbers, with an application to the Entscheidungsproblem\". Proceedings of the London Mathematical Society, vol. s2-42, pp. 230-265, 1936. Albert Einstein. \"On the electrodynamics of moving bodies\". Annalen der Physik, vol. 17, pp. 891-921, 1905. ","permalink":"https://hydro-engineer-sensei.github.io/posts/20251105-1102_math-with-citations/","summary":"This post explores information theory and entropy concepts.","title":"Mathematical Communication Theory"},{"content":"Welcome to my blog! This is my first post using Hugo and PaperMod.\nInline equation: $E = mc^2$\nDisplay equation: $$ \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi} $$\n","permalink":"https://hydro-engineer-sensei.github.io/posts/20251105-1006_my-first-blog-post/","summary":"\u003cp\u003eWelcome to my blog! This is my first post using Hugo and PaperMod.\u003c/p\u003e\n\u003cp\u003eInline equation: $E = mc^2$\u003c/p\u003e\n\u003cp\u003eDisplay equation:\n$$\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}\n$$\u003c/p\u003e","title":"My First Post"}]